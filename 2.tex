\documentclass[stat333]{subfiles}

%% ========================================================
%% document

\begin{document}

    \chap{Conditional Distributions} 

    \section{Jointly Discrete Case}

    \np For convenience, we shall only consider \textit{bivariate} case. Let $X_1,X_2$ be discrete random variables and let $x_2\in\R$ throughout this section.
    
    \begin{definition}{Conditional PMF}{}
        If $p_{X_2}\left( x_2 \right)>0$, then we define the \emph{conditional pmf} of $X_1$ given $X_2=x_2$, denoted as $p_{X_1|X_2}\left( \cdot|x_2 \right)$, is defined by
        \begin{equation*}
            p_{X_1|X_2}\left( x_1|x_2 \right) = \frac{p_{\left( X_1,X_2 \right)}\left( x_1,x_2 \right)}{p_{X_2}\left( x_2 \right)}
        \end{equation*}
        for all $x_1\in\R$. We denote the resulting distribution by $X_1|\left( X_2=x_2 \right)$.
    \end{definition}

    \np 
    \begin{enumerate}
        \item We alternatively write $\PP\left( X_1=\cdot|X_2=x_2 \right)$ to denote $p_{X_1|X_2}\left( \cdot|x_2 \right)$. Also note that
            \begin{equation*}
                p_{X_1|X_2}\left( x_1|x_2 \right) 
                = \PP\left( X_1=x_1 \right)
                = \frac{\PP\left( X_1=x_2,X_2=x_2 \right)}{\PP\left( X_2=x_2 \right)}
                = \frac{p_{\left( X_1,X_2 \right)}\left( x_1,x_2 \right)}{p_{X_2}\left( x_2 \right)}
            \end{equation*}
        \item If $X_1,X_2$ are \textit{independent}, then
            \begin{equation*}
                p_{\left( X_1,X_2 \right)}\left( x_1,x_2 \right) = p_{X_1}\left( x_1 \right)p_{X_2}\left( x_2 \right)
            \end{equation*}
            for every $x_1,x_2\in\R$, which means
            \begin{equation*}
                p_{X_1|X_2}\left( x_1|x_2 \right)=p_{X_1}\left( x_1 \right)
            \end{equation*}
            for all $x_1,x_2\in\R$ such that $p_{X_2}\left( x_2 \right)>0$.
    \end{enumerate}

    \begin{definition}{Conditional Expectation}{}
        If $p_{X_2}\left( x_2 \right)>0$, then we define the \emph{conditional mean}, denoted as $\EE\left( X_1|X_2=x_2 \right)$, of $X_1|\left( X_2=x_2 \right)$ by
        \begin{equation*}
            \EE\left( X_1|X_2=x_2 \right) = \sum^{}_{x_1\in\R:p_{X_1|X_2}\left( x_1|x_2 \right)>0}x_1p_{X_1|X_2}\left( x_1|x_2 \right).
        \end{equation*}
    \end{definition}

    \begin{prop}{}
        Let $w:\R^2\to\R$. Then
        \begin{equation*}
            \EE\left( w\left( X_1,X_2 \right)|X_2=x_2 \right) = \EE\left( w\left( X_1,x_2 \right)|X_2=x_2 \right).
        \end{equation*}
    \end{prop}

    \begin{cor}{}
        Given any $g,h:\R\to\R$,
        \begin{equation*}
            \EE\left( g\left( X_1 \right)h\left( X_2 \right)|X_2=x_2 \right) = \EE\left( g\left( X_1 \right)h\left( x_2 \right)|X_2=x_2 \right).
        \end{equation*}
    \end{cor}	

    \begin{cor}{}
        Let $X_3$ be a random variable and let $x_3\in\R$ be such that $p_{X_3}\left( x_3 \right)>0$. Then
        \begin{equation*}
            \EE\left( X_1+X_2|X_3=x_3 \right) = \EE\left( X_1|X_3=x_3 \right) + \EE\left( X_2|X_3=x_3 \right).
        \end{equation*}
    \end{cor}	

    \begin{definition}{Conditional Variance}{}
        We define the \emph{conditional variance} of $X_1|X_2=x_2$, denoted as $\var\left( X_1|X_2=x_2 \right)$, by
        \begin{equation*}
            \var\left( X_1|X_2=x_2 \right)=\EE\left( \left( X_1-\EE\left( X_1|X_2=x_2 \right) \right)^{2}|X_2=x_2 \right).
        \end{equation*}
    \end{definition}

    \begin{prop}{}
        We have
        \begin{equation*}
            \var\left( X_1|X_2=x_2 \right) = \EE\left( X_1^{2}|X_2=x_2 \right)-\EE\left( X_1|X_2=x_2 \right)^{2}.
        \end{equation*}
    \end{prop}

    \ex Suppose $X_1\sim\binomdis\left( n_1,p \right), X_2\sim\binomdis\left( n_2,p \right)$ for some $n_1,n_2\in\N\cup\left\lbrace 0 \right\rbrace, p\in\left( 0,1 \right]$ are independent and let $m\in\N\cup\left\lbrace 0 \right\rbrace$. Find $p_{X_1|X_1+X_2}\left( \cdot|X_1+X_2=m \right)$.

    \begin{subproof}[Answer]
        We may assume $m\leq n_1+n_2$, since otherwise $p_{X_1+X_2}\left( m \right)=0$. Then observe that
        \begin{flalign*}
            && p_{X_1|X_1+X_2}\left( x_1|X_1+X_2=m \right) & = \frac{\PP\left( X_1=x_1,X_1+X_2=m \right)}{\PP\left( X_1+X_2=m \right)} && \\
            && & = \frac{\PP\left( X_1=x_1,X_2=m-x_1 \right)}{\PP\left( X_1+X_2=m \right)} && \\ 
            && & = \frac{\PP\left( X_1=x_1 \right)\PP\left( X_2=m-x_1 \right)}{\PP\left( X_1+X_2=m \right)}  && \substack{\text{since $X_1,X_2$ are}\\\text{independent}} \\
            && & = \frac{\binom{n_1}{x_1}p^{x_1}\left( 1-p \right)^{n_1-x_1}\binom{n_2}{m-x_1}p^{m-x_1}\left( 1-p \right)^{1-m+x_1}}{\binom{n_1+n_2}{m}p^m\left( 1-p \right)^{1-m}} && \substack{\text{since}\\X_1+X_2\sim\binomdis\left( n_1+n_2,p \right)}
        \end{flalign*} 
        for all $x_1\in\left\lbrace 0,\ldots,n_1\right\rbrace$. But note that this is exactly the pmf of $\hypdis\left( n_1+n_2,n_1,m \right)$. That is,
        \begin{equation*}
            X_1|\left( X_1+X_2=m \right)\sim\hypdis\left( n_1+n_2,n_1,m \right). \eqqedsym
        \end{equation*}
    \end{subproof}

    \noindent Here is an intuitive explanation of why $X_1|\left( X_1+X_2=m \right)\sim\hypdis\left( n_1+n_2,n_1,m \right)$. Consider a sequence of $n_1+n_2$ Bernoulli trials $\left( B_{i} \right)^{n_1+n_2}_{i=1}$, each with success probability $p$. We know exactly $m$ of $B_1,\ldots,B_{n_1+n_2}$ are successes, and we also know exactly $n_1$ of $B_1,\ldots,B_{n_1}$ are successes. But each $B_i$ has success probability $p$, so we end up with a hypergeometric distribution. See (1.11).

    \ex Let $X_1\sim\poidis\left( \lambda_1 \right),\ldots,X_m\sim\poidis\left( \lambda_m \right)$ for some $\lambda_1,\ldots,\lambda_m>0$ be independent and let $Y=\sum^{m}_{i=1} X_i$. Find the conditional distribution of $X_j|\left( Y=n \right)$, where $j\in\left\lbrace 1,\ldots,m \right\rbrace,n\in\N$.

    \begin{subproof}[Answer]
        First note that $X_j, \sum^{m}_{i=1,i\neq j}X_i$ are independent, since $X_1,\ldots,X_m$ are independent. Fix $x_j\in\left\lbrace 0,\ldots,n \right\rbrace$. Then
        \begin{flalign*}
            && p_{X_j|Y}\left( x_j|n \right)& = \frac{\PP\left( X_j=x_j ,Y=n\right)}{\PP\left( Y=n \right)} && \\ 
            && & = \frac{\PP\left( X_j=x_j, \sum^{m}_{i=1}X_i=n \right)}{\PP\left( Y=n \right)} && \\
            && & = \frac{\PP\left( X_j=x_j, \sum^{m}_{i=1,i\neq j} X_i=n-x_j \right)}{\PP\left( Y=n \right)} && \\
            && & = \frac{\PP\left( X_j=x_j \right)\PP\left( \sum^{m}_{i=1,i\neq j}X_i=n-x_j \right)}{\PP\left( Y=n \right)}. && \substack{\text{since $X_j, \sum^{m}_{i=1,i\neq j}X_i$}\\\text{are independent}}\\
        \end{flalign*} 
        But
        \begin{equation}
            Y\sim\poidis\left( \sum^{m}_{i=1}\lambda_i \right), \sum^{m}_{i=1,i\neq j}X_i\sim\poidis\left( \sum^{m}_{i=1,i\neq j}\lambda_i \right)
        \end{equation}
        as sums of random variables, so
        \begin{flalign*} 
            && p_{X_j|Y}\left( x_j|n \right)& = \frac{\frac{e^{-\lambda_j}\lambda_j^{x_j}}{x_j!}\frac{e^{-\sum^{m}_{i=1,i\neq j}\lambda_i}\left( \sum^{m}_{i=1,i\neq j}\lambda_i \right)^{n-x_j}}{\left( n-\lambda_j \right)!}}{\frac{e^{\sum^{m}_{i=1}\lambda_i}\left( \sum^{m}_{i=1}\lambda_i \right)^{n}}{n!}}. && \substack{\text{by [2.1]}}\\
            && & = \binom{n}{x_j} \frac{\lambda_j^{x_j}\left( \sum^{m}_{i=1,i\neq j}\lambda_i \right)^{n-x_j}}{\left( \sum^{m}_{i=1}\lambda_i \right)^n} && \\
            && & = \binom{n}{x_j} \frac{\lambda_j^{x_j}\left( \lambda-\lambda_j \right)^{n-x_j}}{\lambda^n} && \substack{\text{by letting}\\\lambda=\sum^{m}_{i=1}\lambda_i} \\
            && & = \binom{n}{x_j} \left( \frac{\lambda_j}{\lambda} \right)^{x_j}\left( \frac{\lambda-\lambda_j}{\lambda} \right)^{n-x_j} && \\
            && & = \binom{n}{x_j} \left( \frac{\lambda_j}{\lambda} \right)^{x_j}\left( 1-\frac{\lambda_j}{\lambda} \right)^{n-x_j} && \\
            && & = \binom{n}{x_j}p^{x_j}\left( 1-p \right)^{n-x_j}. && \substack{\text{by letting}\\p=\frac{\lambda_i}{\lambda}}
        \end{flalign*} 
        Since $0<\lambda_i\geq\lambda$, $p\in\left( 0,1 \right]$, so it follows that
        \begin{equation*}
            X_j|\left( Y=n \right)\sim\binomdis\left( n,\frac{\lambda_j}{\sum^{m}_{i=1}}\lambda_i \right). \eqqedsym
        \end{equation*}
    \end{subproof}

    \section{Jointly Continuous Case}

    \np Let $X,Y$ be jointly continuous random variables and let $y\in\R$ throughout this section.
    
    \begin{definition}{Conditional PDF}{}
        We define the \emph{conditional pdf} of $X$ given $Y=y$, denoted as $f_{X|Y}\left( \cdot|y \right)$, by
        \begin{equation*}
            f_{X|Y}\left( x|y \right) = \frac{f\left( x,y \right)}{f\left( y \right)}
        \end{equation*}
        for all $x\in\R$.
    \end{definition}

    \np Given $a,b\in\R, a\leq b$, observe that
    \begin{equation*}
        \PP\left( a\leq X\leq b|Y=y \right) = \int^{b}_{a}f_{X|Y}\left( x|y \right)\dx.
    \end{equation*}

    \clearpage
    \begin{definition}{Conditional Expectation}{}
        We define the \emph{conditional expectation} of $X$ given $Y=y$, denoted as $\EE\left( X|Y=y \right)$, as
        \begin{equation*}
            \EE\left( X|Y=y \right) = \int^{\infty}_{-\infty}xf_{X|Y}\left( x|y \right)\dx.
        \end{equation*}
    \end{definition}

    \begin{prop}{}
        Let $g:\R\to\R$. Then
        \begin{equation*}
            \EE\left( g\left( X \right)|Y=y \right)=\int^{\infty}_{-\infty}g\left( x \right)f_{X|Y}\left( x|y \right)\dx.
        \end{equation*}
    \end{prop}

    \begin{definition}{Conditional Variance}{}
        We define the \emph{conditional variance} of $X$ given $Y=y$, denoted as $\var\left( X|Y=y \right)$, as
        \begin{equation*}
            \var\left( X|Y=y \right)=\EE\left( \left( X-\EE\left( X|Y=y \right) \right)^{2}|Y=y \right).
        \end{equation*}
    \end{definition}

    \begin{prop}{}
        We have
        \begin{equation*}
            \var\left( X|Y=y \right)=\EE\left( X^{2}|Y=y \right)-\EE\left( X|Y=y \right)^{2}.
        \end{equation*}
    \end{prop}

    \section{Conditioning}

    \np Let $X,Y$ be random variables. Then we can define $v:\R\to\R$ by
    \begin{equation*}
        v\left( y \right) = \EE\left( X|Y=y \right)
    \end{equation*}
    for all $y\in\R$.

    \begin{notation}{$\EE\left( X|Y \right)$}{}
        Consider the setting of (2.7). We write $\EE\left( X|Y \right)$ to denote $v\left( Y \right)$.
    \end{notation}

    \noindent Since any real-valued function of a random variable is a random variable, so it makes sense to consider the expectation of $\EE\left( X|Y \right)$:
    \begin{equation}
        \EE\left( \EE\left( X|Y \right) \right) =
        \begin{cases} 
            \sum^{}_{y\in\R:p_Y\left( y \right)>0} \EE\left( X|Y=y \right)p_Y\left( y \right) & \text{if $Y$ is discrete} \\
            \int^{\infty}_{-\infty}\EE\left( X|Y=y \right)f_Y\left( y \right)\dy & \text{if $Y$ is continuous}
        \end{cases}.
    \end{equation}

    \begin{theorem}{Law of Total Expectation}
        Let $X,Y$ be random variables. Then
        \begin{equation*}
            \EE\left( X \right) = \EE\left( \EE\left( X|Y \right) \right).
        \end{equation*}
    \end{theorem}

    \begin{proof}
        We shall consider the continuous case only --- assume $X,Y$ are jointly continuous. Recall from the definition of $\EE\left( X|Y \right)$ that
        \begin{equation*}
            \EE\left( \EE\left( X|Y \right) \right)=\int^{\infty}_{-\infty}\EE\left( X|Y=y \right)f_Y\left( y \right)\dy.
        \end{equation*}
        But
        \begin{flalign*}
            && \EE\left( X|Y=y \right) & = \int^{\infty}_{-\infty}xf_{X|Y}\left( x|y \right)\dx && \\ 
            && & = \int^{\infty}_{-\infty}x \frac{f_{\left( X,Y \right)}\left( x,y \right)}{f_Y\left( y \right)}\dx.
        \end{flalign*} 
        It follows that
        \begin{flalign*}
            && \EE\left( \EE\left( X|Y \right) \right)&=\int^{\infty}_{-\infty}\int^{\infty}_{-\infty}xf_{X|Y}\left( x|y \right)\dx f_Y\left( y \right)\dy && \\ 
            && & = \int^{\infty}_{-\infty}\int^{\infty}_{-\infty}x \frac{f_{\left( X,Y \right)}\left( x,y \right)}{f_Y\left( y \right)}\dx f_Y\left( y \right)\dy && \\
            && & = \int^{\infty}_{-\infty}\int^{\infty}_{-\infty}xf_{\left( X,Y \right)}\left( x,y \right)\dx\dy && \\
            && & = \int^{\infty}_{-\infty}x\int^{\infty}_{-\infty}f_{\left( X,Y \right)}\left( x,y \right)\dy\dx && \\
            && & = \int^{\infty}_{-\infty}xf_X\left( x \right)\dx && \\
            && & = \EE\left( X \right),
        \end{flalign*} 
        as desired.
    \end{proof}

    \np Suppose $X\sim\geodis_t\left( p \right)$ where $p\in\left( 0,1 \right]$. Calculate $\EE\left( X \right),\var\left( X \right)$ using the law of total expectation.

    \begin{subproof}[Answer]
        Recall that $X$ is the number of iid Bernoulli trials, each with success probability $p$, needed to obtain the first success. So let $Y$ be the first trial. Then observe that
        \begin{equation*}
            X|\left( Y=1 \right)=1
        \end{equation*}
        but
        \begin{equation*}
            X|\left( Y=0 \right)=X+1.
        \end{equation*}
        By the law of total expeectation,
        \begin{flalign*}
            && \EE\left( X \right) & = \EE\left( \EE\left( X|Y \right) \right)=p_Y\left( 0 \right)\EE\left( X|Y=0 \right)+p_Y\left( 1 \right)\EE\left( X|Y=1 \right) && \\ 
            && & = \left( 1-p \right)\EE\left( X+1 \right)+p\EE\left( 1 \right)=\left( 1-p \right)+\left( 1-p \right)\EE\left( X \right)+p = 1+\left( 1-p \right)\EE\left( X \right),
        \end{flalign*} 
        so rearranging gives
        \begin{equation*}
            \EE\left( X \right)=\frac{1}{p}.
        \end{equation*}
        On the other hand,
        \begin{flalign*}
            && \EE\left( X^{2} \right)&=\EE\left( \EE\left( X^{2}|Y \right) \right)=p_Y\left( 0 \right)\EE\left( X^{2}|Y=0 \right)+p_Y\left( 1 \right)\EE\left( X^{2}|Y=1 \right) && \\ 
            && & = \left( 1-p \right)\EE\left( X^{2}+2x+1 \right)+p\EE\left( 1 \right)=\left( 1-p \right)\EE\left( X^{2} \right)+2\left( 1-p \right)\EE\left( X \right)+1,
        \end{flalign*} 
        so
        \begin{equation*}
            \EE\left( X^{2} \right)=\frac{2\left( 1-p \right)\EE\left( X \right)+1}{p}=\frac{\frac{2-p}{p}+1}{p} = \frac{2}{p^{2}} - \frac{1}{p} + \frac{1}{p} = \frac{2}{p^{2}}.
        \end{equation*}
        Thus
        \begin{equation*}
            \var\left( X \right)=\EE\left( X^{2} \right)-\EE\left( X \right)^{2}=\frac{2}{p^{2}}-\left( \frac{1}{p} \right)^{2}=\frac{1}{p^{2}}. \eqqedsym
        \end{equation*}
    \end{subproof}

    \noindent Note that the obtained expectation and variance agree with the known results.

    \begin{notation}{$\var\left( X|Y \right)$}{}
        Let $X,Y$ be random variables. Let $v:\R\to\R$ be defined by
        \begin{equation*}
            v\left( y \right)=\var\left( X|Y=y \right)
        \end{equation*}
        for all $y\in\R$. Then we write $\var\left( X|Y \right)$ to denote $v\left( Y \right)$.
    \end{notation}

    \np Similar to $\EE\left( X|Y \right)$, $\var\left( X|Y \right)$ is a random variable as a function, $v$, of a random variable, $Y$. The following is a \textit{variance analogue} of the law of total probability.

    \begin{theorem}{Conditional Variance Formula}
        Let $X,Y$ be random variables. Then
        \begin{equation*}
            \var\left( X \right)=\EE\left( \var\left( X|Y \right) \right)+\var\left( \EE\left( X|Y \right) \right).
        \end{equation*}
    \end{theorem}

    \begin{proof}
        First note that, for any $y\in\R$,
        \begin{equation*}
            \var\left( X|Y=y \right)=\EE\left( X^{2}|Y=y \right)-\EE\left( X|Y=y \right)^{2},
        \end{equation*}
        which means
        \begin{equation*}
            \var\left( X|Y \right)=\EE\left( X^{2}|Y \right)-\EE\left( X|Y \right)^{2}.
        \end{equation*}
        On the other hand,
        \begin{equation*}
            \var\left( \EE\left( X|Y \right) \right) = \EE\left( \EE\left( X|Y \right)^{2} \right)-\EE\left( \EE\left( X|Y \right) \right)^{2}.
        \end{equation*}
        It follows from the law of total expectation that
        \begin{equation*}
            \EE\left( \var\left( X|Y \right) \right)+\var\left( \EE\left( X|Y \right) \right) = \EE\left( \EE\left( X^{2}|Y \right) \right)-\EE\left( \EE\left( X|Y \right) \right)^{2}=\EE\left( X^{2} \right)-\EE\left( X \right)^{2}=\var\left( X \right).\eqedsym
        \end{equation*}
    \end{proof}

    \ex[Random Sum]Let $\left( X_{i} \right)^{\infty}_{i=1}$ be an iid sequence of random variables with common mean $\mu\in\R$ and common variance $\sigma^{2}\geq 0$ and let $N$ be a nonnegative integer-valued random variable that is independent of $X_1,\ldots$. Let
    \begin{equation*}
        T = \sum^{N}_{i=1}X_i.
    \end{equation*}
    Find $\EE\left( T \right),\var\left( T \right)$.

    \begin{subproof}[Answer]
        By the law of total probability,
        \begin{flalign*}
            && \EE\left( T \right)&=\EE\left( \EE\left( T|N \right) \right)=\EE\left( \EE\left( T|N=n \right)|_{n=N} \right) =\EE\left( \EE\left( \sum^{N}_{i=1}X_i|N=n \right)|_{n=N} \right) && \\
            && & = \EE\left( \EE\left( \sum^{n}_{i=1}X_i|N=n \right)|_{n=N} \right) = \EE\left( \EE\left( \sum^{n}_{i=1}X_i \right)|_{n=N} \right) = \EE\left( \sum^{N}_{i=1}X_i \right)&& \\ 
            && & = \EE\left( \mu N \right) = \mu\EE\left( N \right).
        \end{flalign*} 
        Moreover,
        \begin{equation*}
            \var\left( T|N=n \right) = \var\left( \sum^{N}_{i=1}X_i|N=n \right)=\var\left( \sum^{n}_{i=1}X_i|N=n \right)=\var\left( \sum^{n}_{i=1}X_i \right) = n\sigma^{2},
        \end{equation*}
        which means
        \begin{equation*}
            \EE\left( \var\left( T|N \right) \right)=\EE\left( N\sigma^{2} \right)=\sigma^{2}\EE\left( N \right).
        \end{equation*}
        On the other hand,
        \begin{equation*}
            \var\left( \EE\left( T|N \right) \right)=\var\left( \mu N \right)=\mu^{2}\var\left( N \right).
        \end{equation*}
        Thus
        \begin{equation*}
            \var\left( T \right)=\EE\left( \var\left( T|N \right) \right)+\var\left( \EE\left( T|N \right) \right)=\sigma^{2}\EE\left( N \right)+\mu^{2}\var\left( N \right)
        \end{equation*}
        by the conditional variance formula.
    \end{subproof}

    \np Recall from [2.2] that, given any random variables $X,Y$,
    \begin{equation*}
        \EE\left( \EE\left( X|Y \right) \right) =
        \begin{cases} 
            \sum^{}_{y\in\R:p_Y\left( y \right)>0} \EE\left( X|Y=y \right)p_Y\left( y \right) & \text{if $Y$ is discrete} \\
            \int^{\infty}_{-\infty}\EE\left( X|Y=y \right)f_Y\left( y \right)\dy & \text{if $Y$ is continuous}
        \end{cases}.
    \end{equation*}
    Now, suppose that $A$ represents some event of interest and we desire to determine $\PP\left( A \right)$. Define an \textit{indicator random variable} $X$ by
    \begin{equation*}
        X = 
        \begin{cases} 
            0 &\text{if }A^{C}\text{ occurs} \\
            1 &\text{if }A\text{ occurs} \\
        \end{cases}.
    \end{equation*}
    Clearly, $\PP\left( X=1 \right)=\PP\left( A \right), \PP\left( X=0 \right)=1-\PP\left( A \right)$, so that $X\sim\berdis\left( \PP\left( A \right) \right)$. Hence $\EE\left( X \right)=\PP\left( A \right)$ and
    \begin{flalign*}
        && \EE\left( X|Y=y \right)& = \sum^{}_{x\in\left\lbrace 0,1 \right\rbrace}x\PP\left( X=x|Y=y \right) && \\ 
        && & = 0\PP\left( X=0|Y=y \right)+1\PP\left( X=1|Y=y \right) && \\
        && & = \PP\left( X=1|Y=y \right) && \\
        && & = \PP\left( A|Y=y \right).
    \end{flalign*} 
    for any random variable $Y$. Hence [2.2] becomes
    \begin{equation}
        \PP\left( A \right)=
        \begin{cases} 
            \sum^{}_{y\in\R:p_Y\left( y \right)>0} \EE\left( A|Y=y \right)p_Y\left( y \right) & \text{if $Y$ is discrete} \\
            \int^{\infty}_{-\infty}\EE\left( A|Y=y \right)f_Y\left( y \right)\dy & \text{if $Y$ is continuous}
        \end{cases}
    \end{equation}
    for all random variable $Y$.

    \ex Let $X,Y$ be independent continuous random variables. Show that
        \begin{equation}
            \PP\left( X<Y \right) = \int^{\infty}_{-\infty}F_X\left( y \right)f_Y\left( y \right)\dy.
        \end{equation}

    \begin{subproof}
        Let $A$ be the event
        \begin{equation*}
            A = \left\lbrace X<Y \right\rbrace.
        \end{equation*}
        Then we have
        \begin{flalign*}
            && \PP\left( X<Y \right) & = \PP\left( A \right) = \int^{\infty}_{-\infty}\PP\left( A|Y=y \right)f_Y\left( y \right)\dy = \int^{\infty}_{-\infty}\PP\left( X<Y|Y=y \right)f_Y\left( y \right)\dy && \\ 
            && & = \int^{\infty}_{-\infty}\PP\left( X<y|Y=y \right)f_Y\left( y \right)\dy = \int^{\infty}_{-\infty}\PP\left( X<y \right)f_Y\left( y \right)\dy = \int^{\infty}_{-\infty}\PP\left( X\leq y \right)f_Y\left( y \right)\dy && \\
            && & = \int^{\infty}_{-\infty}F_X\left( y \right)f_Y\left( y \right)\dy. &&\fqqedsym
        \end{flalign*} 
    \end{subproof}

    \ex Consider the setting of (EX 2.12) and further assume that $X,Y$ are identically distributed. Show that [2.4] simplifies to
    \begin{equation}
        \PP\left( X<Y \right)=\frac{1}{2}.
    \end{equation}

    \begin{subproof}
        Observe that $f_X=f_Y$ since $X,Y$ are iid, so
        \begin{equation*}
            \PP\left( X<Y \right)=\int^{\infty}_{-\infty}F_X\left( y \right)f_Y\left( y \right)\dy = \int^{\infty}_{-\infty}F_X\left( y \right)f_X\left( y \right)\dy = \int^{1}_{0}u\du = \frac{1}{2}
        \end{equation*}
        by the change of variable $u = F_X\left( y \right)$.
    \end{subproof}

    \ex Suppose $X\sim\expdis\left( \lambda_1 \right),Y\sim\expdis\left( \lambda_2 \right)$ are independent. Show
    \begin{equation}
        \PP\left( X<Y \right)=\frac{\lambda_1}{\lambda_2}.
    \end{equation}

    \begin{subproof}
        Since $X\sim\expdis\left( \lambda_1 \right), Y\sim\expdis\left( \lambda_2 \right)$, we have
        \begin{equation*}
            \begin{cases} 
                f_y\left( y \right)&=\lambda_2e^{-\lambda_2y} \\
                F_X\left( y \right)&=1-e^{-\lambda_1y}
            \end{cases}
        \end{equation*}
        for all $y>0$. It follows from [2.4] that
        \begin{flalign*}
            && \PP\left( X<Y \right)&=\int^{\infty}_{-\infty}F_X\left( y \right)f_Y\left( y \right)\dy = \int^{\infty}_{0}\left( 1-e^{-\lambda_1y} \right) \lambda_2e^{-\lambda_2y}\dy = \lambda_2\int^{\infty}_{0}e^{-\lambda_2y}-e^{-\left( \lambda_1+\lambda_2 \right)y}\dy&& \\ 
            && & = \lambda_2\left.\left( -\frac{1}{\lambda_2}e^{-\lambda_2y}+\frac{1}{\lambda_1+\lambda_2}e^{-\left( \lambda_1+\lambda_2 \right)y} \right)\right|^{\infty}_{y=0}=1-\frac{\lambda_2}{\lambda_1+\lambda_2}=\frac{\left( \lambda_1+\lambda_2 \right)-\lambda_2}{\lambda_1+\lambda_2}=\frac{\lambda_1}{\lambda_1+\lambda_2}. && \fqqedsym
        \end{flalign*} 
    \end{subproof}

    \ex Suppose $W,X,Y$ are positive independent continuous random variables and let $Z=X|\left( X<Y \right)$. Show that
    \begin{flalign*}
        && U & = \left( W,X \right)|\left( W<X<Y \right) && \\
        && V & = \left( W,Z \right)|\left( W<Z \right)
    \end{flalign*} 
    are identically distributed.

    \begin{subproof}
        Observe that
        \begin{equation}
            F_U\left( w,x \right)=\PP\left( W\leq w, X\leq x|W<X<Y \right)= \frac{\PP\left( W\leq w, X\leq x, W<X,X<Y \right)}{\PP\left( W<X,X<Y \right)}
        \end{equation}
        for every $w,x>0$. By conditioning on $X$,
        \begin{flalign*}
            && \PP\left( W<X,X<Y \right)&= \int^{\infty}_{0}\PP\left( W<X,X<Y|X=s \right)f_X\left( s \right)\ds && \\ 
            && & = \int^{\infty}_{0}\PP\left( W<s,s<Y|X=s \right)f_X\left( s \right)\ds && \\
            && & = \int^{\infty}_{0}\PP\left( W<s\right)\PP\left(s<Y \right)f_X\left( s \right)\ds, && \flnumber
        \end{flalign*} 
        where the last equality follows from the fact that $W,X,Y$ are independent. In a similar manner,
        \begin{flalign*}
            && \PP\left( W\leq w, X\leq x, W<X,X<Y \right)&=\int^{\infty}_{0}\PP\left( W\leq w, X\leq x, W<X,X<Y|X=s \right)f_X\left( s \right)\ds && \\ 
            && & = \int^{\infty}_{0}\PP\left( W\leq w, s\leq x, W<s, s<Y|X=s \right)f_X\left( s \right)\ds && \\
            && & = \int^{\infty}_{0}\PP\left( W\leq w \right)\PP\left( s\leq x \right)\PP\left( W<s \right)\PP\left( s<Y \right)f_X\left( s \right)\ds && \\
            && & = \PP\left( W\leq w \right)\int^{x}_{0}\PP\left( W<s \right)\PP\left( s<Y \right)f_X\left( s \right)\ds. && \flnumber
        \end{flalign*} 
        Moreover, for every $z>0$,
        \begin{flalign*}
            && F_Z\left( z \right) & = \PP\left( Z\leq z \right)=\PP\left( X\leq z|X<Y \right)=\frac{\PP\left( X\leq z,X<Y \right)}{\PP\left( X<Y \right)} && \\
            && & = \frac{\int^{\infty}_{0}\PP\left( X\leq z, X<Y|X=s \right)f_X\left( s \right)\ds}{\PP\left( X<Y \right)} = \frac{\int^{\infty}_{0}\PP\left( s\leq z, s<Y|X=s \right)f_X\left( s \right)\ds}{\PP\left( X<Y \right)} && \\
            && & = \frac{\int^{z}_{0}\PP\left( s<Y \right)f_X\left( s \right)\ds}{\PP\left( X<Y \right)},
        \end{flalign*} 
        so by differentiating with respect to $z$, we obtain
        \begin{equation}
            f_Z\left( z \right)=\frac{\dif}{\dz}\frac{\int^{z}_{0}\PP\left( s<Y \right)f_X\left( s \right)\ds}{\PP\left( X<Y \right)} = \frac{\PP\left( z<Y \right)f_X\left( z \right)}{\PP\left( X<Y \right)}.
        \end{equation}
        Now note that the cdf of $V$ is given by
        \begin{equation}
            F_V\left( w,z \right)=\PP\left( W\leq w, Z\leq z|W<Z \right)=\frac{\PP\left( W\leq w, Z\leq z, W<Z \right)}{\PP\left( W<Z \right)}
        \end{equation}
        for every $w,z>0$. Since $W$ independent of $X,Y$, it is independent of $Z=X|\left( X<Y \right)$, so
        \begin{flalign*}
            && \PP\left( W<Z \right)&=\int^{\infty}_{0}\PP\left( W<Z|Z=s \right)f_Z\left( s \right)\dz = \int^{\infty}_{0}\PP\left( W<s|Z=s \right)f_Z\left( s \right)\ds  && \\ 
            && & = \int^{\infty}_{0}\PP\left( W<s \right)f_Z\left( s \right)\ds = \int^{\infty}_{0}\PP\left( W<s \right) \frac{\PP\left( s<Y \right)f_X\left( s \right)}{\PP\left( X<Y \right)}\ds && \\
            && & \overset{\text{[2.8]}}{=} \frac{\PP\left( W<X,X<Y \right)}{\PP\left( X<Y \right)}. && \flnumber
        \end{flalign*} 
        Furthermore,
        \begin{flalign*}
            && \PP\left( W\leq w, Z\leq z, W<Z \right)& = \int^{\infty}_{0}\PP\left( W\leq w, Z\leq z, W<Z|Z=s \right)f_Z\left( s \right)\ds && \\ 
            && & = \int^{\infty}_{0}\PP\left( W\leq w, s\leq z, W<s|Z=s \right)f_Z\left( s \right)\ds && \\
            && & = \PP\left( W\leq w \right)\int^{z}_{0}\PP\left( W<s \right)f_Z\left( s \right)\ds && \\
            && & \overset{\text{[2.10]}}{=} \int^{z}_{0}\PP\left( W<s \right) \frac{\PP\left( Y>s \right)f_X\left( s \right)}{\PP\left( X<Y \right)}\ds &&\\
            && & \overset{\text{[2.9]}}{=} \frac{\PP\left( W\leq w, X\leq x, W<X,X<Y \right)}{\PP\left( X<Y \right)} &&\flnumber
        \end{flalign*} 
        for every $w,z>0$. Thus
        \begin{flalign*}
            && F_V\left( w,z \right)& \overset{\text{[2.11]}}{=} \frac{\PP\left( W\leq w, Z\leq z, W<Z \right)}{\PP\left( W<Z \right)} = \frac{\frac{\PP\left( W\leq w, X\leq z, W<X,X<Y \right)}{\PP\left( X<Y \right)}}{\frac{\PP\left( W<X,X<Y \right)}{\PP\left( X<Y \right)}}&& \\ 
            && & \overset{\substack{\text{[2.12]}\\\text{[2.13]}}}{=} \frac{\PP\left( W\leq w, X\leq z, W<X,X<Y \right)}{\PP\left( W<X,X<Y \right)} \overset{\text{[2.7]}}{=}F_U\left( w,z \right)&& \\ 
        \end{flalign*} 
        for every $w,z>0$, so $V\sim U$.
    \end{subproof}

    \clearpage
    \ex Consider an experiment in which iid trials, each with success probability $p\in\left( 0,1 \right]$, are performed until $k\in\N$ consecutive successes are observed. Dtermine the expectation of the number of trials needed to achieve $k$ consecutive successes.

    \begin{subproof}[Answer]
        For each $l\in\N$, let $N_l$ denote the number of trials required to achieve $l$ consecutive successes, where we desire to find $\EE\left( N_k \right)$. 
        First note that $N_1\sim\geodis\left( p \right)$, so
        \begin{equation}
            \EE\left( N_1 \right)=\frac{1}{p}.
        \end{equation}
        For the general case, the idea is to condition on $N_{l-1}$: fix $l\in\N, l\geq 2$ and observe that
        \begin{equation*}
            \EE\left( N_l \right)=\EE\left( \EE\left( N_l|N_{l-1} \right) \right)
        \end{equation*}
        from the law of total expectation. Define, for every $n\in\N$,
        \begin{equation*}
            Y|\left( N_{l-1}=n \right) = 
            \begin{cases} 
                0 & \text{if $n+1$th trial is a failure} \\
                1 & \text{if $n+1$th trial is a success}
            \end{cases}.
        \end{equation*}
        Then, for every $n\in\N$,
        \begin{flalign*}
            && \EE\left( N_l|N_{l-1} \right) & \overset{\text{[2.3]}}{=} \sum^{}_{y\in\left\lbrace 0,1 \right\rbrace}\EE\left( N_l|N_{l-1}=n, Y=y \right)\PP\left( Y=y|N_{l-1}=n \right)&& \\ 
            && & = p\EE\left( N_l|N_{l-1}=n, Y=1 \right)+\left( 1-p \right)\EE\left( N_l|N_{l-1}=n, Y=0 \right) && \\
            && & = p\left( n+1 \right)+\left( 1-p \right)\left( n+1+\EE\left( N_l \right) \right)&& \\
            && & = n+1+\left( 1-p \right)\EE\left( N_l \right),
        \end{flalign*} 
        since
        \begin{flalign*}
            && N_l|\left( N_{l-1}=n, Y=0 \right)&\sim n+1+N_l, && \\ 
            && N_l|\left( N_{l-1}=n, Y=1 \right)&\sim n+1.
        \end{flalign*} 
        This implies
        \begin{equation*}
            \EE\left( N_l \right) = \EE\left( \EE\left( N_l|N_{l-1} \right) \right) = \EE\left( N_{l-1} \right)+1+\left( 1-p \right)\EE\left( N_l \right),
        \end{equation*}
        so
        \begin{equation}
            \EE\left( N_l \right)= \frac{\EE\left( N_{l-1} \right)+1}{p}.
        \end{equation}
        Now the claim is that 
        \begin{equation}
            \EE\left( N_l \right)=\sum^{l}_{r=1}\frac{1}{p^r}.
        \end{equation}
        To verify this, note that the base case is provided by [2.14]. Moreover, for every $l\in\N, l\geq 2$,
        \begin{equation*}
            \EE\left( N_l \right) = \frac{\sum^{l-1}_{r=1} \frac{1}{p^r} + 1}{p} = \frac{1}{p} + \sum^{l-1}_{r=1} \frac{1}{p^{r+1}} = \sum^{l}_{r=1} \frac{1}{p^r}
        \end{equation*}
        by induction. Thus by [2.16],
        \begin{equation*}
            \EE\left( N_k \right)= \sum^{k}_{r=1} \frac{1}{p^r}. \eqqedsym
        \end{equation*}
    \end{subproof}




































    
    
    
    


\end{document}
